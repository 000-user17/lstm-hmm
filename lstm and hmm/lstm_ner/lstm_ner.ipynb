{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用LSTM做NER，参考了pytorch官网文档\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "filepath = \"train.csv\"\n",
    "train = pd.read_csv(filepath)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "for line in range(0, len(train[\"text\"])):   \n",
    "    temp1, temp2 = [one for one in train[\"text\"][line]], train[\"BIO_anno\"][line].split()   #将字符个数和bio个数不等的去掉\n",
    "    if len(temp1) == len(temp2):\n",
    "        training_data.append(([one for one in train[\"text\"][line]], train[\"BIO_anno\"][line].split()))\n",
    "\n",
    "for line in range(0, len(test[\"text\"])):\n",
    "    testing_data.append([one for one in test[\"text\"][line]])\n",
    "\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "#print(word_to_ix)\n",
    "\n",
    "for sent in testing_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B-BANK\": 0, \"I-BANK\": 1, \"B-PRODUCT\": 2, \"I-PRODUCT\": 3, \"O\": 4, \"B-COMMENTS_N\": 5, \"I-COMMENTS_N\": 6, \"B-COMMENTS_ADJ\": 7, \"I-COMMENTS_ADJ\": 8}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 32  #要增大embedding dim，否则超出index报错\n",
    "HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentens, tags in training_data:    #判断是否由个数不相等的\n",
    "    if len(sentens) != len(tags):\n",
    "        print('error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2054\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)).to(device) #将模型转到gpu上运行\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "\n",
    "for epoch in range(100):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix).to(device)  #将输入数据弄到gpu上\n",
    "        targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-BANK', 'I-BANK', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_N', 'O', 'B-COMMENTS_N', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "model = model.to('cpu')  #再将模型转为cpu运行\n",
    "tag_scores_all = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0,len(training_data)):\n",
    "        inputs = prepare_sequence(training_data[i][0], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_scores_all.append(tag_scores)\n",
    "        #将scores保存在tag_scores_all中\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    #print(tag_scores_all)\n",
    "    \n",
    "ans_all = []  #存储训练集预测的scores矩阵\n",
    "#训练集预测\n",
    "for num in range(0,len(training_data)):\n",
    "    max_indexs = []\n",
    "    ans = []\n",
    "    for l in range(tag_scores_all[num].shape[0]):  \n",
    "        #获取scores数组中的最大index，即可以转化称为命名实体\n",
    "        list1 = list(tag_scores_all[num][l])\n",
    "        max_val = max(tag_scores_all[num][l])\n",
    "        max_index = list1.index(max_val)\n",
    "        max_indexs.append(max_index)  #一句话的对应的最大索引\n",
    "    for index in max_indexs:\n",
    "        ans.append(list(tag_to_ix.keys())[index])\n",
    "    ans_all.append(ans)#所有text对应的命名实体\n",
    "\n",
    "print(ans_all[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-COMMENTS_ADJ', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-PRODUCT', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "tag_scores_test = []#存储测试集预测的scores矩阵\n",
    "with torch.no_grad():\n",
    "    for i in range(0,len(test[\"text\"])):\n",
    "        inputs = prepare_sequence(testing_data[i], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_scores_test.append(tag_scores)\n",
    "\n",
    "\n",
    "ans_test = []\n",
    "for num in range(0,len(test[\"text\"])):\n",
    "    max_indexs = []\n",
    "    ans = []\n",
    "    for l in range(tag_scores_test[num].shape[0]):  \n",
    "        #获取scores数组中的最大index，即可以转化称为命名实体\n",
    "        list1 = list(tag_scores_test[num][l])\n",
    "        max_val = max(tag_scores_test[num][l])\n",
    "        max_index = list1.index(max_val)\n",
    "        max_indexs.append(max_index)  #一句话的对应的最大索引\n",
    "    for index in max_indexs:\n",
    "        ans.append(list(tag_to_ix.keys())[index])\n",
    "    ans_test.append(ans)#所有text对应的命名实体\n",
    "print(ans_test[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O I-COMMENTS_ADJ O O O O B-PRODUCT I-PRODUCT O B-PRODUCT I-PRODUCT I-PRODUCT O B-PRODUCT O B-COMMENTS_N I-COMMENTS_N O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "BIO_anno = str(ans_test[0])\n",
    "BIO_anno = BIO_anno.replace(',','')\n",
    "BIO_anno = BIO_anno.replace('\\'','')\n",
    "BIO_anno = BIO_anno.strip('[')\n",
    "BIO_anno = BIO_anno.strip(']')\n",
    "print(BIO_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cs-demo\\AppData\\Local\\Temp/ipykernel_67392/986796269.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  algo_output['id'][i] = int (algo_output['id'][i])  #string转换为int\n",
      "C:\\Users\\cs-demo\\AppData\\Local\\Temp/ipykernel_67392/986796269.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  algo_output['class'][i] = int (algo_output['class'][i])\n"
     ]
    }
   ],
   "source": [
    "#用ans_test改变之前的标注栏目\n",
    "len(ans_test)\n",
    "test_update = pd.read_csv('output.csv')\n",
    "file = open('output1.csv', 'w', encoding='utf8')\n",
    "\n",
    "out = 'id' + ',' + 'BIO_anno' + ',' + 'class'+'\\n'\n",
    "file.write(str(out))\n",
    "\n",
    "for line in range(0, len(test_update['BIO_anno'])):\n",
    "    BIO_anno = str(ans_test[line])\n",
    "    BIO_anno = BIO_anno.replace('\\'','')\n",
    "    BIO_anno = BIO_anno.replace(',','')\n",
    "    BIO_anno = BIO_anno.strip('[')\n",
    "    BIO_anno = BIO_anno.strip(']')\n",
    "    out = str(test_update['id'][line]) + ',' + BIO_anno + ',' + str(test_update['class'][line])+'\\n'\n",
    "    file.write(str(out))\n",
    "\n",
    "algo_output=pd.read_csv('output1.csv')\n",
    "for i in range(0,len(algo_output[\"id\"])):\n",
    "    algo_output['id'][i] = int (algo_output['id'][i])  #string转换为int\n",
    "    algo_output['class'][i] = int (algo_output['class'][i])\n",
    "file.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed2b4d9c6d10d022777493e9ba813f87e78014dfbcffa4a004578ae8b0a5cb2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
